{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Week 3: the Twitter API\n\nIn this lesson, we're going to learn how to analyze and explore Twitter data with the Python/command line tool [twarc](https://twarc-project.readthedocs.io/en/latest/). We're¬†specifically going to work with [twarc2](https://twarc-project.readthedocs.io/en/latest/twarc2/), which is designed for version 2 of the Twitter API (released in 2020) and the Academic Research track of the Twitter API (released in 2021), which enables researchers to collect tweets from the entire Twitter archive for free.\n\nTwarc was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n\n> This notebook builds upon the work of [Melanie Walsh](https://melaniewalsh.github.io/Intro-Cultural-Analytics/), released under a [Creative Commons BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) License. This notebook shares the same license.",
   "metadata": {
    "tags": [],
    "cell_id": "cef899c3-43f0-4ab2-8c00-6eef0e2636d9",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 340.921875
   }
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-warning alert\">\n\n**Note1:** There is a major difference in the workflow throughout this exercise. It is because the way `twarc2` is designed. It is a command-line tool that needs to be run from a terminal shell. In a Jupyter notebook, we can run these shell commands directely from the notebook prefixing them with `!`.\n\nFor example, you might have used `ls` command in the terminal to list the contents of the directory. Try running `!ls` from this notebook now. \n</div>\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00001-bbf430c9-5119-4305-ace7-206122e9ed40",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 229.0625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ad8f3ca4",
    "execution_start": 1647003486441,
    "execution_millis": 290,
    "output_cleared": false,
    "cell_id": "00002-4bbde1bd-e23e-460a-8061-54bfe5ab9c79",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 112
   },
   "source": "!ls",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "week-3-twitter.ipynb  week-3-twitter-solutions.ipynb\r\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-warning alert\">\n\n**Note2:** The commands that we run using `!twarc2` will result in a file being saved in the local directory. You can check this file through the file browser towards the left hand side. We will use `pandas` to read the resulting file. \n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00003-9696c758-33b3-4e43-9433-2c3001e229cd",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 147.859375
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Workflow\n\nWe will need to install three command line tools:\n- the program `twarc2` to access Twitter API, in order to archive tweets, counts, user metadata.\n- `twarc-csv`, a plugin to convert the tweets archived by `twarc2` into a CSV format.\n- `twarc-hashtags`, a plugin to analyze hashtag counts in the tweets retrieved by `twarc2`.\n\nNote: the program `twarc2` requires the Python package `twarc` (with no 2). The number ‚Äò2‚Äô is for the Twitter API v2.\n",
   "metadata": {
    "tags": [],
    "cell_id": "00004-248dd9f4-94ae-4dd4-813b-a79c615493d6",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 246.71875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4b2b74b9",
    "execution_start": 1647003486766,
    "execution_millis": 30591,
    "output_cleared": true,
    "cell_id": "00005-af8d2bc6-63f1-43d5-8f6c-b01dad2a63ed",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135
   },
   "source": "%%capture\n!pip install twarc --upgrade\n!pip install twarc-csv --upgrade\n!pip install twarc-hashtags --upgrade",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Configure and set up warc2\n\nOnce twarc2 is installed, you need to configure it with your API keys and/or bearer token so that you can actually access the API.\n\nIf you are running this notebook in DeepNote, select the terminal icon on the left hand side. If you are running this notebook in your local computer or on server, you will need to open a terminal. \n\nRun the following command in the terminal \n```\ntwarc2 configure\n\n```\n\n\nTwarc will ask for your bearer token, which you can copy and paste into the blank after the colon, and then press enter. You can optionally enter your API keys, as well.\n\nIf you‚Äôve entered your information correctly, you should get a congratulatory message that looks something like this:\n\n```\nYour keys have been written to /Users/<your username>/Library/Application Support/twarc/config\n\n‚ú® ‚ú® ‚ú®  Happy twarcing! ‚ú® ‚ú® ‚ú®\n\n```\n\n<br />\n\nNow you‚Äôre ready to collect and analyze tweets!",
   "metadata": {
    "tags": [],
    "cell_id": "00006-51ff6bba-b3db-4227-a9a0-e6fe6ef48fc0",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 545.71875
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Archive tweets matching a query\n\nTo collect tweets from the Twitter API, we need to form a query and ask twarc2 to download all the tweets that match it: `twarc2 search *query*`. The simplest kind of query is a keyword search, such as the phrase ‚ÄúOxford Internet Institute,‚Äù which should return any tweet that contains all of these words in any order ‚Äî `twarc2 search \"Oxford Internet Institute\"`.",
   "metadata": {
    "tags": [],
    "cell_id": "00007-d3d1bf2c-c560-4f3d-b961-d7636ca53d88",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 174.921875
   }
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 0.1:** Retrieve the tweets that contain the keywords (in any order) \"Oxford Internet Institute\" and save them to `tweets-oii.json`\n\n</div>\n",
   "metadata": {
    "tags": [],
    "cell_id": "00008-031ad507-f0f6-479b-9bab-833afbd8bb0a",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 126
   }
  },
  {
   "cell_type": "markdown",
   "source": "To output Twitter data to a file, we include a filename with the ‚Äú.jsonl‚Äù file extension, which stands for JSON lines, a special kind of JSON file:",
   "metadata": {
    "tags": [],
    "cell_id": "00009-947a4c20-89c1-4f6d-9686-8b8ce6abc581",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 74.125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cbce5e7e",
    "execution_start": 1647003517365,
    "execution_millis": 2940,
    "output_cleared": false,
    "cell_id": "00010-20faa43e-7fb6-4424-838a-685dc2331076",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 292
   },
   "source": "!twarc2 search \"Oxford Internet Institute\" tweets-oii.jsonl",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "\r\nüëã  Hi I don't see a configuration file yet, so let's make one.\r\n\r\nPlease follow these steps:\r\n\r\n1. visit https://developer.twitter.com/en/portal/\r\n2. create a project and an app\r\n3. go to your Keys and Tokens and generate your keys\r\n\r\nPlease enter your Bearer Token (leave blank to skip to API key configuration): ",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 0.2:** Convert these tweets into a `csv` file named `tweet-oii.csv`.\n\nUse `pandas` in Python to load that CSV file and count the number of tweets returned by the search.\n</div>\n",
   "metadata": {
    "tags": [],
    "cell_id": "00011-75f79f76-48b5-4541-b42d-deb7e818e1a2",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 161.859375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "48451602",
    "execution_start": 1646577167644,
    "execution_millis": 2398,
    "output_cleared": true,
    "cell_id": "00012-08c252f0-a37e-418b-b6ea-6aad0c670b6e",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "!twarc2 csv tweets-oii.jsonl tweets-oii.csv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "ffe61c6",
    "execution_start": 1646577170053,
    "execution_millis": 257,
    "output_cleared": true,
    "cell_id": "00013-3625854b-c3f8-448d-9ba2-54ca4da38e7d",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135
   },
   "source": "import pandas as pd \ntweets = pd.read_csv(\"tweets-oii.csv\")\n\ntweets.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 0.3:** Next, we are going to rename a number of columns to make the data more readable. we are going to rename the columns. Look at the columns and find which columns represent `date of the tweet`,`number of retweets`, `number of likes`, `number of quotes`,  `number of replies`, `author twitter handle`, `author name`, `author's twitter bio`, `text of the tweet`,  `whether the author is verified or not`.\n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00014-df2b7c34-13d0-4d1f-97f0-5f44d6ca253b",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 192.65625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "19167285",
    "execution_start": 1646577170388,
    "execution_millis": 920756,
    "output_cleared": true,
    "cell_id": "00015-f6deaacd-ac13-4da3-b0bb-0f0938ea51e1",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "print(tweets.columns)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "c6db9fbf",
    "execution_start": 1646577170402,
    "execution_millis": 917688,
    "output_cleared": true,
    "cell_id": "00016-ffa164da-55ad-4656-a8a2-082a95739b7b",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 387
   },
   "source": "def rename_dataframe_tweets(dataframe):\n    \"\"\" Rename the columns of a dataset archived by twarc2. \"\"\"\n    return dataframe.rename(columns={\n        'created_at': 'date',\n        'public_metrics.retweet_count': 'retweets', \n        'author.username': 'username', \n        'author.name': 'name',\n        'author.verified': 'verified', \n        'public_metrics.like_count': 'likes', \n        'public_metrics.quote_count': 'quotes', \n        'public_metrics.reply_count': 'replies',\n        'author.description': 'user_bio'\n    })\n\ntweets = rename_dataframe_tweets(tweets)\n\n# note we are keeping the column `text` as it is. \ntweets = tweets[['date', 'retweets', 'username', 'name', 'verified', 'likes', 'quotes', 'replies', 'user_bio', 'text']]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Now we can view our more focused DataFrame!",
   "metadata": {
    "tags": [],
    "cell_id": "00017-e60d12e4-fbce-462e-be2e-1232754b0f66",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "d36541de",
    "execution_start": 1646577170415,
    "execution_millis": 907715,
    "output_cleared": true,
    "cell_id": "00018-6a43c8aa-d171-4968-ab4f-2841c7ca0d1e",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "tweets.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 0.4:** Let's have a look at the text of these tweets. Print 10 tweets from the dataframe. \n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00019-9f1f67b6-6ce0-4ae5-bd70-d0d9a7d9ebd8",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 104
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "58c9579f",
    "execution_start": 1646577170433,
    "execution_millis": 2618688,
    "output_cleared": true,
    "cell_id": "00020-8c38e2fe-022f-4f53-a19a-8a68995a3e4d",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "for t in tweets.text[:10]:\n    print(t)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 0.5:** Analyze  the hashtags in the above tweets and save the resulting counts of hashtags in a CSV file named `hashtags-oii.csv`. Read this csv file in `pandas` and check the counts of these hashtags.\n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00021-334b1168-f2b1-427b-a026-8dbf391e1a2b",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 147.859375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "688fdd94",
    "execution_start": 1646577170444,
    "execution_millis": 2458,
    "output_cleared": true,
    "cell_id": "00022-9c744968-aaae-4d62-96e4-4a2d552efd4d",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "!twarc2 hashtags tweets-oii.jsonl hashtags-oii.csv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "efbd23fc",
    "execution_start": 1646577172905,
    "execution_millis": 38,
    "output_cleared": true,
    "cell_id": "00023-39352436-cd56-4347-9995-ec2cab485e0d",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "hashtags_oii = pd.read_csv('hashtags-oii.csv')\nhashtags_oii",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Get Tweets (Academic Track, Full Twitter Archive)\n\nSo far we have been querying the standard Twitter API that limits the search results to the past 7 days. In order to search the full Twitter archive, we only need to add a single flag of `--archive` to our `twarc2 search` command so that the command now looks like `twarc2 search *query* --archive filename`. \n\n<div class=\"alert-info alert\">\n\n**Exercise 1:** Now repeat the Exercise 0.1, 0.2, and 0.3 by running the search query on the entire archive.\n\nFor the sake of clarity, add `-archive` to the .jsonl output file. For example, save the tweets in `tweet-oii-archive.jsonl`. \n\n</div>\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00024-5f19cce6-84e9-45b4-b8e2-a4dbad81f89d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 383.859375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "6fb3b41",
    "execution_start": 1646577172965,
    "execution_millis": 100643,
    "output_cleared": true,
    "cell_id": "00025-c2e842d1-e6f6-4af3-9f3c-d00779195073",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135
   },
   "source": "# To limit the timit your query will take to run, let's add --start-time 2020-01-01 to limit tweets to 2020‚Äî2022.\n\n!twarc2 search --archive --start-time 2020-01-01 \"Oxford Internet Institute\" tweets-oii-archive.jsonl \n!twarc2 csv tweets-oii-archive.jsonl tweets-oii-archive.csv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "a0f04a0d",
    "execution_start": 1646577273618,
    "execution_millis": 460,
    "output_cleared": true,
    "cell_id": "00026-7faf2153-1b26-41bf-8098-49a4958d5d15",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "tweets = pd.read_csv(\"tweets-oii-archive.csv\")\n\ntweets = rename_dataframe_tweets(tweets)  # We reuse the function we defined above\n\ntweets = tweets[['date', 'retweets', 'username', 'name', 'verified', 'likes', 'quotes', 'replies', 'user_bio']]\nprint(f\"We collected {tweets.shape[0]} tweets!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "f925722e",
    "output_cleared": true,
    "execution_start": 1646577274074,
    "execution_millis": 3542,
    "cell_id": "00027-e9c7afc2-dcbd-42f5-b074-ac1d64dfc49b",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "!twarc2 hashtags tweets-oii-archive.jsonl hashtags-oii-archive.csv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "6bbdfd85",
    "output_cleared": true,
    "execution_start": 1646577277632,
    "execution_millis": 370,
    "cell_id": "00028-f71d7c55-3db4-4af9-8f4d-8691fce8937b",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "hashtags_oii = pd.read_csv('hashtags-oii-archive.csv')\nhashtags_oii",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Advanced Search of Twitter API\n\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00029-4e188647-d50d-4f95-93be-92ce6b3e39d1",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "markdown",
   "source": "The Twitter API is very complex and not everything can be taught in this class. Our aim is to show you the basis but also get you comfortable in constructing more complex queries.\n\nThere are many other operators that we can add to a query, which would allow us to collect tweets only from specific Twitter users or locations, or to only collect tweets that meet certain conditions, such as containing an image or being authored by a verified Twitter user. Here‚Äôs an table of the main search operators taken from the Twitter documentation (https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list). If you're interested, look also at:\n\n- Twarc's own documentation: https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/\n- Some advanced examples from `twitterdev` Github account: https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/5-how-to-write-search-queries.md\n\n<hr />\n<br />\n\n| Search Operator      | Explanation | Example |\n|:---------------------|:------------|:------- |\n| `keyword`              | Matches a keyword within the body of a Tweet. |`so sweet and so cold` |                    \n| `\"exact phrase match\"` | Matches the exact phrase within the body of a Tweet. | `\"so sweet and so cold\" OR \"plums in the icebox\"` |\n| `-`                    | Do NOT match a keyword or operator | `birthday -happy`, `oxford -university` |\n| `#`                    | Matches any Tweet containing a recognized hashtag.  | `#arthistory` |\n| `from:`, `to:`         | Matches any Tweet from or to a specific user. | `from:cynddl` `to:cynddl` |\n| `place:`               | Matches Tweets tagged with the specified location or Twitter place ID. | `place:\"new york city\" OR place:london` |\n| `is:reply`, `is:quote` | Returns only replies or quote tweets. | `thank you is:reply` ` from:OxfordUCU is:quote` |\n| `is:verified`          | Returns only Tweets whose authors are verified by Twitter. | `buy NFT is:verified` |\n| `has:media`            | Matches Tweets that contain a media object, such as a photo, GIF, or video, as determined by Twitter. | `My new song is out has:media` |\n| `has:images`, `has:videos` | Matches Tweets that contain a recognized URL to an image. | `the view from my window has:images` |\n| `has:geo`              | Matches Tweets that have Tweet-specific geolocation data provided by the Twitter user. | `honeymoon has:geo` |\n",
   "metadata": {
    "tags": [],
    "cell_id": "00030-4a1950d6-0048-4af1-84a0-cca135c9eac4",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 861.265625
   }
  },
  {
   "cell_type": "markdown",
   "source": "\n### Query construction\n\n- To construct a query of two keywords with **logical AND**, use a space. So if the search term is `corona` and `coronavirus`, our query will be `\"corona coronavirus\"`\n- To construct a query of two keywords with **logical OR**, we use OR as a keyword. So if the search term is `corona`or `coronavirus` our query will be `\"corona OR coronavirus\"`\n- To construct a query to **match exact phrase**, we enclose that phrase in quotation marks. So if the search term is `covid` OR `corona virus`, our query will be `'corona OR \"corona virus\"'`. **NOTE** how we have enclosed the query in single quotes because the inner phrase needs to use double quotes.\n- To construct a **complex query** which has `keyword1` and either of `keyword2` or `keyword1`, we can enclose the OR condition in circular parenthesis. Thus, our query will be `\"keyword1 (keyword2 OR keyword3)\"`.\n\n### Filters\n\nThe table above presented a few filters you can add to your queries. For a full list, please refer to the API documentation. For instance:\n\n- To filter the tweets by authors who are verified by Twitter (journalists, artists, politicians, etc.), add `is:verified` to your query.  For example, to search for `keyword1` only from authors who are verified, our query will be ``keyword1 is:verified\"`\n- To filter by tweets that have some form of media, our query will look like `\"keyword1 has:media\"`\n\n### Other command-line options\n\nThere are two options that are specific to `twarc` so you will have to add them to your command instead to your query.\n\n- **Search limit:** if you want to limit your query to just 500 tweets, the command will look like `twarc2 search \"query\" --limit 500 output.jsonl`\n\n- **Time range:** if you want to limit your search reesults to a time range, the command will look like `twarc2 search --start-time 2014-07-17 --end-time 2014-07-24 \"query\" output.jsonl`\n\n\nLet's get used to all this through an example. ",
   "metadata": {
    "tags": [],
    "cell_id": "00031-e78af485-e136-46a5-a074-e469189cd5fd",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 809.921875
   }
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 2.1:** We will search for posts related to Russian invasion of Ukraine and the conflict in Ukraine. Since the volume of tweets containing `Ukraine` or `Russia` is too much to be dealt in the duration of the class, we will narrow down our search using keywords and filters.\n\nLet's construct a query with the following specifications:\n- contains exact words/phrases: either `Russia` or `–†–æ—Å—Å–∏—è` and either `Ukraine` or `–£–∫—Ä–∞–∏–Ω–∞`\n- is tweeted only by verified authors\n- contains hashtags\n- has media\n- is not a retweet (`-is:retweet`).\n\nWe will further limit our search results based on the following criteria\n- Maximum 50,000 tweets\n- Start date be 2022-02-27\n- End date be 2022-02-28\n\nStore the output in `tweets-ukr-rus.jsonl`.\n</div>\n",
   "metadata": {
    "tags": [],
    "cell_id": "00032-46a9f3c1-fb80-4f99-bd00-14818ab2312d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 484.921875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "output_cleared": true,
    "deepnote_to_be_reexecuted": true,
    "source_hash": "d4f80ae8",
    "execution_start": 1646588429010,
    "execution_millis": 308,
    "cell_id": "00033-b6c6f4bf-3ee5-4a75-8d2e-1975cb3a8ede",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "!twarc2 search --archive --limit 50000 --start-time 2022-02-27 --end-time 2022-02-28 \"(Russia OR –†–æ—Å—Å–∏—è) (Ukraine OR –£–∫—Ä–∞–∏–Ω–∞) is:verified -is:retweet has:media has:hashtags\" tweets-ukr-rus.jsonl",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 2.2:** Write the `twarc2` command to convert the `jsonl` file to a CSV file.\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00034-71a56dde-427a-4940-8a9e-2c326cfbc930",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 104
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "e5c77d5d",
    "execution_start": 1646577373275,
    "execution_millis": 11613,
    "output_cleared": true,
    "cell_id": "00035-93b9efad-4174-48ea-aee1-b66cd598b102",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "!twarc2 csv tweets-ukr-rus.jsonl tweets-ukr-rus.csv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 2.3:** Read the above CSV file using pandas\n\n- Look at the columns and find which columns represent `date of the tweet`,`number of retweets`, `number of likes`, `number of quotes`,  `number of replies`, `author twitter handle`, `author name`, `author's twitter bio`, `whether the author is verified or not`\n- Let's rename to `date`, `retweets`, `likes`, `quotes`, `replies`, `username`, `name`, `user_bio`, `verified`.\n- Let's keep only these columns as well as the main text of the tweet.\n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00036-b2b33f5e-f8f0-452e-821c-f358b1d21775",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 280.859375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "5f857e3c",
    "execution_start": 1646578607824,
    "execution_millis": 487,
    "output_cleared": true,
    "cell_id": "00037-d3636bec-5279-4dd9-b154-a5c952da84cd",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "tweets = pd.read_csv(\"tweets-ukr-rus.csv\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "a897c8bc",
    "execution_start": 1646577385474,
    "execution_millis": 49,
    "output_cleared": true,
    "cell_id": "00038-0915cc42-266c-42ab-b2a2-57ea041f633e",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "source": "tweets = rename_dataframe_tweets(tweets)  # We reuse the function we defined above\n\ntweets = tweets[['date', 'retweets', 'username', 'name', 'verified', 'likes', 'quotes', 'replies', 'user_bio', 'text']].copy()\n\ntweets.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 2.4:**  What are the most popular tweets? Let's sort them:\n\n- in descending order of the number of  likes;\n- in descending order of the number of  retweets.\n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00039-1576ce8a-4622-42b9-9dd7-d162e18a83d9",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 165.53125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "fdbed2c1",
    "execution_start": 1646577385563,
    "execution_millis": 88,
    "output_cleared": true,
    "cell_id": "00040-7b31aeab-85b2-4186-b072-6ee48c184107",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "tweets.sort_values(\"likes\", ascending=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "5a072465",
    "execution_start": 1646577385664,
    "execution_millis": 96,
    "output_cleared": true,
    "cell_id": "00041-a5593432-222f-429a-ab4a-4ccebc666276",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "tweets.sort_values(\"retweets\", ascending=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 2.5:**  Let's plot the hourly frequency of tweets.\n\n- Check the type of elements that `date` column has. Are they `str` or `datatime` objects? If they are `str`, we need to convert them to `datetime` objects so that we can do the necessary aggregation. We will use `pd.to_datetime` to do that.\n- We will use a handy functionality in pandas to do a frequency count on datetime objects. To do that, we need to set the index of dataframe as `date` column.\n- We will use `resample` functionality of `pandas` to resample the datetime index into bins according to the specification. For example, if you want to plot daily frequency, pass the argument `'D'`, and if you want to plot hourly frequency, pass the argument `'H'`. Call `.resample('H')` on the dataframe obtained above.\n- In order to aggregate these results, we will use `.size()` function on the resampled dataframe.\n- Finally, we will plot the resulting counts using `.plot()`, a handy pandas method that calls the matplotlib library.\n\n</div>\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00042-d79e8459-6321-4252-83aa-9af71c0e6364",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 421.921875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "5ee87929",
    "execution_start": 1646577385808,
    "execution_millis": 10,
    "output_cleared": true,
    "cell_id": "00043-4ccae076-2289-4f61-9274-b0bfc54cd4ee",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "# What does the date column contains? A series of string objects.\nprint(tweets['date'].loc[0])\nprint(type(tweets['date'].loc[0])) ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "aebf7707",
    "execution_start": 1646577385809,
    "execution_millis": 1,
    "output_cleared": true,
    "cell_id": "00044-335696b1-69bb-423f-b805-701a1327c849",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "# Let's convert it to a pandas datetime series:\ntweets['date'] = pd.to_datetime(tweets['date'])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "370260d8",
    "execution_start": 1646577385810,
    "execution_millis": 43,
    "output_cleared": true,
    "cell_id": "00045-ee540e75-dd65-495e-9295-1b36c4beb7e6",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "# We convert the dataframe to have the date column as an ‚Äòindex‚Äô\ntweets_by_date = tweets.copy()\ntweets_by_date.set_index('date', inplace=True)\n\n# We now make use of pandas' advanced timeseries processing tools: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html\nhourly_tweets = tweets_by_date.resample('H')\nhourly_frequency_tweets = hourly_tweets.size()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "2cfce037",
    "execution_start": 1646577385853,
    "execution_millis": 1875,
    "output_cleared": true,
    "cell_id": "00046-bda9b5a9-9b84-4084-a659-e1d4cd48d6bc",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "hourly_frequency_tweets.plot(title=\"Hourly volume of tweets\", rot=45);",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-warning alert\">\n\nAlthough pandas takes time to learn, you eventually gain a lot of times due to the advanced functions it provides.\n\nThis **resampling technique is very easy to transfer** for other sort of plots:\n- you can adjust the window from years to minutes. For instance, resampling with the rule `'D'` returns days, `'10T'` returns bins of 10 minutes‚Ä¶\n- you can use it to compute statistics on any columns, such as average number of likes for instance.\n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00047-608a9e3a-0ddf-4830-9445-7cfff59423c7",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 269.125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "a4525950",
    "execution_start": 1646577684691,
    "execution_millis": 724,
    "output_cleared": true,
    "cell_id": "00048-89773c20-0446-477b-bb8b-f5c97db55916",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "hourly_tweets = tweets_by_date.resample('H')\nhourly_tweets.likes.mean().plot(title=\"Hourly average number of likes\", rot=45);",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n\n**Exercise 2.7:** Plot the volume of retweets every 30 minutes.\n\n</div>",
   "metadata": {
    "cell_id": "e88be6b8-bdea-4ac3-abc6-a0733c8c264c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 104
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "aba9d422-e209-4412-8df2-b8ef761267dc",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "76951b1c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "tweets_by_date.resample('30T').retweets.sum().plot(title=\"# of retweets every 30 minutes\", rot=45)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 2.8:**  Use `twarc` to output summary of hashtags in the retrieved tweets and save the final output in CSV format at `hashtags-ukr-rus.csv`\n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00049-6fe2f054-15cc-4098-b841-81cec9e73d84",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 126
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "d4ac14a9",
    "execution_start": 1646577388079,
    "execution_millis": 4176,
    "output_cleared": true,
    "cell_id": "00050-0356a5c2-5f59-40dd-bd58-34917f0a3082",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "!twarc2 hashtags tweets-ukr-rus.jsonl hashtags-ukr-rus.csv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 2.9:**  Read the above CSV file and retrieve the top 10 hastags from there. \n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00051-33d60221-d5d1-4f92-b873-21a5f9be4a1b",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 104
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "954f4bcd",
    "execution_start": 1646577666254,
    "execution_millis": 20993,
    "output_cleared": true,
    "cell_id": "00052-7384cb59-2815-4cbd-aa56-43b0c2f8437f",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "hashtags_ukraine = pd.read_csv(\"hashtags-ukr-rus.csv\")\nhashtags_ukraine.columns",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "67a4dfb6",
    "execution_start": 1646577392009,
    "execution_millis": 30,
    "output_cleared": true,
    "cell_id": "00053-ef0f8679-7f70-4b4f-8df6-df2686eb9d5c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "hashtags_ukraine.sort_values(\"tweets\", ascending=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Optional: archiving only counts of tweets",
   "metadata": {
    "tags": [],
    "cell_id": "00054-3baeadcd-e19d-4d39-8bcb-5bb9f26309f4",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "markdown",
   "source": "Some of the queries we tested above quickly take time. If you only want to collect aggregated counts of tweets, disagregated by days for instance, there's an easier way. ",
   "metadata": {
    "tags": [],
    "cell_id": "00055-1f40f41e-92fc-493b-aea9-9b8697c5537a",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 74.125
   }
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert alert-info\">\n\n**Exercise 3.1:** Execute the twarc2 command below to count the number of tweets that match a given query.\n\nWe will directly obtain the counts by day, in a CSV format.\n</div>\n",
   "metadata": {
    "tags": [],
    "cell_id": "00056-53f0d821-f928-4615-8720-c8d122dae458",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 161.859375
   }
  },
  {
   "cell_type": "markdown",
   "source": "**Note:** To call `twarc` for counts, we execute the following command:\n\n```\n!twarc2 counts \"query\" --csv --granularity day > filename.csv\n```\n\n<br />\n\nHere, `--csv` specifies the desired output format, `--granularity` specifies the aggregation level, and \nthe keyword `>` redirects the output of the command before this keyword to the file mentioned after this keyword. ",
   "metadata": {
    "tags": [],
    "cell_id": "00057-0cbc1349-ba3d-4662-8cf3-f946a38050f7",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 198.328125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "34eee9b5",
    "execution_start": 1646577392046,
    "execution_millis": 2791,
    "output_cleared": true,
    "cell_id": "00058-643f9929-9c20-4900-af5d-98a2ecaea8a9",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "!twarc2 counts '(Russia OR –†–æ—Å—Å–∏—è) (Ukraine OR –£–∫—Ä–∞–∏–Ω–∞) is:verified has:media has:hashtags ' --csv --granularity day > counts-ukr-rus.csv ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-info alert\">\n\n**Exercise 3.2:** Now use pandas to read the CSV file that you saved above. \n\n- How many columns are there? What does these column mean?\n- Plot the daily counts using pandas plot function : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00059-b02e5ef2-9969-43c3-81a4-19b3ab055b6e",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 187.921875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "69e3c793",
    "execution_start": 1646577394855,
    "execution_millis": 1,
    "output_cleared": true,
    "cell_id": "00060-73d749ef-2afc-4cb8-8657-1f6a028b79b3",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "counts_ukr_rus = pd.read_csv(\"counts-ukr-rus.csv\")\nprint(counts_ukr_rus.columns)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "bdf9dfa9",
    "execution_start": 1646577394884,
    "execution_millis": 27,
    "output_cleared": true,
    "cell_id": "00061-dfc24850-fbb5-4bb1-a4db-69b369579b0c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "counts_ukr_rus.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "7e10c700",
    "execution_start": 1646577394914,
    "execution_millis": 5,
    "output_cleared": true,
    "cell_id": "00062-2100edca-034b-4ba8-a7ee-46c56b571310",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "# Let's convert the start and end series to datetime\ncounts_ukr_rus['start'] = pd.to_datetime(counts_ukr_rus['start'])\ncounts_ukr_rus['end'] = pd.to_datetime(counts_ukr_rus['end'])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b0844f38",
    "execution_start": 1646577730182,
    "execution_millis": 604,
    "output_cleared": true,
    "cell_id": "00063-0a962b01-5864-4b38-8841-620b37405823",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "counts_ukr_rus.plot(y=\"day_count\", x=\"start\", rot=45, marker='x', legend=False, ylabel=\"Daily tweet volume\", xlabel=\"Day\");",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert-warning alert\">\n\n**For more examples** of accessing data from Twitter API, check out `twarc`'s documentation: https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/\n\nYou can notably use it to access the followers and friends (mutual followers) of a given user.\n\nIf you're interested in mapping networks, have a look at another plugin, twarc-network: https://github.com/DocNow/twarc-network\n\n</div>",
   "metadata": {
    "tags": [],
    "cell_id": "00064-4c78fe4b-53f7-4d63-94cf-ea2ee2d9b235",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 220.65625
   }
  },
  {
   "cell_type": "markdown",
   "source": "## This week's datasheet questions",
   "metadata": {
    "tags": [],
    "cell_id": "00065-ad17ea4b-edff-4daf-b4af-d3f46f6da021",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "markdown",
   "source": "Throughout this course, we will aim to build on the practice of documenting our datasets, using the Datasheet for Datasets framework (here is an <a href=\"https://github.com/zykls/folktables/blob/main/datasheet.md\">example of a datasheet\"</a>). In the notebook for this week, you designed a small dataset of tweets related to the conflict in Ukraine. Let's assume you plan to release this dataset online.\n\nHow would you structure your Datasheet for this small dataset? For this week's homework, please answer the following questions:",
   "metadata": {
    "tags": [],
    "cell_id": "00066-a97d256a-55d8-41c2-92b4-91faf390a25c",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 177.71875
   }
  },
  {
   "cell_type": "markdown",
   "source": ">**How was the data associated with each instance acquired?** Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n\n...\n\n>**If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?**\n\n...\n\n>**Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?** If not, please describe the timeframe in which the data associated with the instances was created.\n\n...\n\n>**Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?** If so, please describe why.\n\n...\n\n>**Did the individuals in question consent to the collection and use of their data?** If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\n\n...\n",
   "metadata": {
    "tags": [],
    "cell_id": "00067-da581c20-b3e5-41b2-953b-facc1cbd088d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 625.71875
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f746e373-dc41-4dbe-b3f9-5f3af42ff658' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "82bc054c-251d-4069-bb98-d34ff316dabf",
  "deepnote_execution_queue": []
 }
}